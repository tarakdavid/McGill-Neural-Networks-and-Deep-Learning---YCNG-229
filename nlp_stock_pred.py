# -*- coding: utf-8 -*-
"""nlp_stock_pred.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D7Oae6MG323C8bmaS8wqurgSABeS1KSs
"""

!pip install vaderSentiment

!pip install transformers
!pip install SentencePiece

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration
from transformers import pipeline
sentiment_analysis = pipeline('sentiment-analysis')

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix

from google.colab import files
uploaded = files.upload()

headlines = pd.read_csv('DowJ_News.csv')
values = pd.read_csv('DowJ_Figures.csv')

#let's analyze data
headlines.shape

values.shape

headlines.head()

values.head()

#lets create a column knowing the future close date; then create labels from that, that way we have labels based on known close

values['Adj Close Tomorrow'] = values['Adj Close'].shift(-1)
values['Label*'] = values.apply(lambda x: 1 if (x['Adj Close Tomorrow'] >= x['Adj Close']) else 0, axis=1)

values.head()

#lets merge both datasets

full = headlines.merge(values, on='Date')

full.head()

#now we want all headlines together in one column, this way we can perform nlp on it

merged_lines = []
for row in range(0, len(full.index)):
  merged_lines.append(' '.join(str(x) for x in full.iloc[row, 2:27]))

merged_lines[3]

#now we wan to clean text, remove the b and \'
clean_merged = []
for i in range(0,len(merged_lines)):
  clean_merged.append(re.sub("b[(')]", '', merged_lines[i]))
  clean_merged[i] = re.sub('b[(")]', '', clean_merged[i])
  clean_merged[i] = re.sub("\'",'', clean_merged[i])

clean_merged[3]

full['Combined News'] = clean_merged

full.head()

#drop some columns to retain data we want to work with
full_revised = full[['Date','Label*','Combined News','Open','Close','Adj Close','Volume']]

full_revised.head()

full_test = full_revised

full_test.head() #polarity -1 to 1; subjectivity 0-1, 1=subjective. but look more into explanation

#get sentiment by utilizing vader
def obt_SIA(text):
  vader = SentimentIntensityAnalyzer()
  sentiment = vader.polarity_scores(text)
  return sentiment

compound = [] #sum of all lexicon ratings which have been normalized between -1 and+1
positive = []
neutral = []
negative = []
SIA = 0

for i in range(0,len(full_revised['Combined News'])):
  SIA = obt_SIA(full_revised['Combined News'][i])
  compound.append(SIA['compound'])
  positive.append(SIA['pos'])
  neutral.append(SIA['neu'])
  negative.append(SIA['neg'])

full_test['Compound'] = compound
full_test['Positive'] = positive
full_test['Neutral'] = neutral
full_test['Negative'] = negative

full_test.head()

full_test['Date'] = pd.to_datetime(full_test.Date).dt.date

full_test.head()

from google.colab import files
full_test.to_csv('full_clean.csv') 
files.download('full_clean.csv')

plt.plot(full_test['Date'],full_test['Adj Close'])
plt.show()

plt.plot(full_test['Date'],full_test['Compound'])
plt.show()

#Lets do some visualization

fig, ax1 = plt.subplots()
   
color = 'tab:blue'
ax1.set_xlabel('Date (s)')
ax1.set_ylabel('Close', color = color)
ax1.plot(full_test['Date'], full_test['Adj Close'], color = color)
ax1.tick_params(axis ='y', labelcolor = color)
   
ax2 = ax1.twinx()
   
color = 'tab:green'
ax2.set_ylabel('Compound', color = color)
ax2.plot(full_test['Date'], full_test['Compound'], color = color)
ax2.tick_params(axis ='y', labelcolor = color)
  
fig.suptitle('Close Price vs Compound', fontweight ="bold")
plt.show()

#add confusion matrix to models

X = full_test
X = np.array(X.drop(['Label*','Date','Combined News'], axis =1))
Y = np.array(full_test['Label*'])

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=0)

# normalize X's
x_train = preprocessing.scale(x_train)
x_test = preprocessing.scale(x_test)

    # test models
models = {  'LinearDiscriminantAnalysis':LinearDiscriminantAnalysis(),
                'SVM Classification': SVC(),
                'SGDClassifier': SGDClassifier(loss="hinge", penalty="l2", max_iter=75),
                'RandomForestClassifier': RandomForestClassifier(n_estimators=75)
                }

for modeltype in models.keys():

    model = models[modeltype]
    print('* * *',modeltype,'* * *')
    model.fit(x_train,y_train)
    print(classification_report(model.predict(x_test),y_test))
    print(accuracy_score(model.predict(x_test),y_test))
    print(confusion_matrix(y_test,model.predict(x_test)))

comparetest = full_test
comparetest.head()

comparetest.shape

model_name = "human-centered-summarization/financial-summarization-pegasus"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)
modeltf = TFPegasusForConditionalGeneration.from_pretrained(model_name)

comparetest['Combined News'].iloc[1]

input_ids = tokenizer.encode(comparetest['Combined News'].iloc[0], return_tensors='pt')
output = model.generate(input_ids, max_length=55, num_beams=5, early_stopping=True)
summary = tokenizer.decode(output[0], skip_special_tokens=True)

summary

summaries = []
for i in range(0,101):
  input_ids = tokenizer.encode(comparetest['Combined News'].iloc[i], return_tensors='tf')
  output = modeltf.generate(input_ids, max_length=71, num_beams=5, early_stopping=True)
  summary = tokenizer.decode(output[0], skip_special_tokens=True)
  summaries.append(summary)

summaries

sentiments = sentiment_analysis(summaries)

type(sentiments)

compare101 = comparetest.head(101)

compare101.shape

compare101['Transformers Sentiment'] = sentiments

compare101.head()

def find_sentiment(text):
    if vader.polarity_scores(text)["compound"] > 0:
        return "POSITIVE"
    elif vader.polarity_scores(text)["compound"] < 0:
        return "NEGATIVE"
    else:
        return "NEUTRAL"

vader = SentimentIntensityAnalyzer()

vader_sent = find_sentiment(summaries)

compare101['Vader Sentiment'] = vader_sent

compare101.head(21)

from google.colab import files
compare101.to_csv('compare_nlp.csv') 
files.download('compare_nlp.csv')